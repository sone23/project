{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a3509b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas\n",
    "import math\n",
    "from keras import Model\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Input, Attention\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Layer\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras import backend as K\n",
    "\n",
    "from keras.layers import SimpleRNN, BatchNormalization, LeakyReLU\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.ensemble import RandomForestRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b3053bb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       ...,\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_data():\n",
    "    dataset = read_csv('solar_AL.txt', header=0, usecols = [5], engine='python')\n",
    "    dataset[:] = numpy.nan_to_num(dataset)    \n",
    "    return dataset.values\n",
    "\n",
    "def create_dataset(dataset, look_back=1, look_ahead=0):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset)-look_back - look_ahead-1):\n",
    "        a = dataset[i:(i+look_back), 0]\n",
    "        \n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i + look_back + look_ahead, 0])\n",
    "    return numpy.array(dataX), numpy.array(dataY)\n",
    "load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e850ada7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "(42047, 1) (10512, 1)\n"
     ]
    }
   ],
   "source": [
    "look_back = 1\n",
    "look_ahead = 0\n",
    "# Use multiple scalers for different columns / features\n",
    "# \n",
    "dataset = load_data()\n",
    "print(dataset)\n",
    "\n",
    "scaler_train = MinMaxScaler(feature_range=(-1, 1))\n",
    "scaler_test = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "\n",
    "train_size = int(len(dataset) * 0.80)\n",
    "test_size = len(dataset) - train_size\n",
    "train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n",
    "print(train.shape, test.shape)\n",
    "\n",
    "train = scaler_train.fit_transform(train)\n",
    "test = scaler_test.fit_transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8381b18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model():\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(4, input_shape=(1, look_back)))\n",
    "    model.add(Dense(16, activation = 'tanh', kernel_initializer = 'he_normal'))\n",
    "    model.add(Dense(8, activation = 'tanh', kernel_initializer = 'he_normal'))\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    \n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab87632c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1314/1314 [==============================] - 43s 23ms/step - loss: 0.2924\n",
      "1314/1314 [==============================] - 18s 14ms/step - loss: 0.2885\n",
      "1314/1314 [==============================] - 18s 14ms/step - loss: 0.2885\n",
      "1314/1314 [==============================] - 11s 9ms/step - loss: 0.2885\n",
      "1314/1314 [==============================] - 13s 10ms/step - loss: 0.2885\n",
      "1314/1314 [==============================] - 12s 9ms/step - loss: 0.2885\n"
     ]
    }
   ],
   "source": [
    "gen_losses = []\n",
    "disc_losses = []\n",
    "for look_ahead in range(0, 50):\n",
    "    trainX, trainY = create_dataset(train, look_back, look_ahead)\n",
    "    testX, testY = create_dataset(test, look_back, look_ahead)\n",
    "#     print(trainX.shape, [testX].shape, testY.shape)\n",
    "    trainX = numpy.reshape(trainX, (trainX.shape[0], look_back, trainX.shape[1]))\n",
    "    testX = numpy.reshape(testX, (testX.shape[0], look_back, testX.shape[1]))\n",
    "    generator = make_model()\n",
    "    discriminator = make_model()\n",
    "    generator_optimizer = tf.keras.optimizers.Adam(1e-6)\n",
    "    discriminator_optimizer = tf.keras.optimizers.Adam(1e-3)\n",
    "    \n",
    "    for i in range(10):\n",
    "        with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "            generated_values = generator(trainX, training=True)\n",
    "#             generator.fit(trainX, trainY, epochs=1, batch_size=1, verbose=2)\n",
    "#             generated_values = generator(trainX)\n",
    "#             decision = discriminator(trainX, training=True)\n",
    "            discriminator.fit(trainX, trainY - generated_values)\n",
    "            decision = generator(trainX)\n",
    "        \n",
    "            gen_loss = tf.sqrt(tf.losses.mean_squared_error(trainY, generated_values))\n",
    "            disc_loss = tf.sqrt(tf.losses.mean_squared_error(trainY - generated_values, decision))\n",
    "\n",
    "        gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "        gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "        generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "#         discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "    \n",
    "    testPredict = generator(testX, training = False)\n",
    "    discPredict = discriminator(testX, training = False)\n",
    "    #reshape testY to the shape wanted\n",
    "    # Create a numpy array with testY \n",
    "    print(testY.shape, testPredict.shape)\n",
    "    ge_score = round(math.sqrt(mean_squared_error([testY][0], testPredict)), 3)\n",
    "    print(\"Gen Score: \", ge_score)\n",
    "    gen_losses.append(ge_score)\n",
    "    \n",
    "    di_score = round(math.sqrt(mean_squared_error([testY][0], discPredict + testPredict)), 3)\n",
    "    print(\"Disc Score: \", di_score)\n",
    "    disc_losses.append(di_score)\n",
    "#     print(testPredict.shape)\n",
    "#     plt.plot(dataset[train_size + look_ahead + look_back: -1 ], label = \"Ground\")\n",
    "    plt.plot([testY][0], label = \"Test\")\n",
    "    plt.plot(testPredict, label = \"Prediction\")\n",
    "    plt.plot((discPredict + testPredict), label = \"Disc Predict\")\n",
    "    plt.title(\"Plot for \" + str(look_ahead))\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plt.plot(gen_losses, label = \"Gen Losses\")\n",
    "plt.plot(disc_losses, label = \"Disc Losses\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949f7249",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
